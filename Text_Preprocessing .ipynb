{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Text Mining</center>\n",
    "\n",
    "___\n",
    "\n",
    "Text data falls into the category of unstructured data and requires some preparation before it can be used for modeling. Text preperation is different from structed data pre-processing.\n",
    "\n",
    "Today we will go through the process of preparing text data and building a predictive model on it.\n",
    "\n",
    "## Why SpaCy?\n",
    "\n",
    "There are many different libraries that can be used for text related processing. We will work with SpaCy.\n",
    "\n",
    ">SpaCy is a free and open-source library developed by Explosion AI. It works well for simple to complex language understanding tasks and is designed specifically for production use.\n",
    "\n",
    "SpaCy provides trained models for 48 differnt languages and has a model for multi-language as well. \n",
    "\n",
    ">Check this link for various English models : https://spacy.io/models/en\n",
    "\n",
    "Before jumping in, let's have a look at various features provided by popular NLP related libraries and their performance in compairision to SpaCy.  \n",
    "_*All charts are referenced from SpaCy Docs*_\n",
    "\n",
    "### Feature Comparision\n",
    "\n",
    "![](feature-comparision.png)\n",
    "\n",
    "### Speed Comparision\n",
    "\n",
    "![](speed-comparision.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Installation\n",
    "\n",
    "To get started with SpaCy, install the package using pip in Terminal (for Mac) or CommandLine (for Windows)\n",
    "\n",
    "The language pre-trained model packages can be downloaded using the \"spacy download\" command. We will download `en_core_web_md` package\n",
    "\n",
    "- en = English\n",
    "- core = Core (Vocab, Syntax, Entities, Vectors)\n",
    "- web = Web Text\n",
    "- sm/md/lg = Small/Medium/Large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the library is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T09:31:19.060023Z",
     "start_time": "2019-11-16T09:31:19.040500Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-2.3.2-cp37-cp37m-win_amd64.whl (9.3 MB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.3-cp37-cp37m-win_amd64.whl (32 kB)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.8.0-py3-none-any.whl (23 kB)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Using cached srsly-1.0.2-cp37-cp37m-win_amd64.whl (179 kB)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Using cached catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.2-cp37-cp37m-win_amd64.whl (20 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda\\lib\\site-packages (from spacy) (4.42.1)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Using cached plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting thinc==7.4.1\n",
      "  Downloading thinc-7.4.1-cp37-cp37m-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda\\lib\\site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from spacy) (45.2.0.post20200210)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda\\lib\\site-packages (from spacy) (1.18.1)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.2-cp37-cp37m-win_amd64.whl (105 kB)\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "  Using cached blis-0.4.1-cp37-cp37m-win_amd64.whl (5.0 MB)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in d:\\anaconda\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\anaconda\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (2.2.0)\n",
      "Installing collected packages: cymem, wasabi, srsly, catalogue, murmurhash, plac, preshed, blis, thinc, spacy\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.3 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 spacy-2.3.2 srsly-1.0.2 thinc-7.4.1 wasabi-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_md==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.3.1/en_core_web_md-2.3.1.tar.gz (50.8 MB)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in d:\\anaconda\\lib\\site-packages (from en_core_web_md==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in d:\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.18.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in d:\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.22.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (45.2.0.post20200210)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in d:\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in d:\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (4.42.1)\n",
      "Requirement already satisfied: thinc==7.4.1 in d:\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in d:\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2019.11.28)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in d:\\anaconda\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\anaconda\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.2.0)\n",
      "Building wheels for collected packages: en-core-web-md\n",
      "  Building wheel for en-core-web-md (setup.py): started\n",
      "  Building wheel for en-core-web-md (setup.py): finished with status 'done'\n",
      "  Created wheel for en-core-web-md: filename=en_core_web_md-2.3.1-py3-none-any.whl size=50916646 sha256=52f6d361f5a880ffb3b223e632599ec1ce6ed58c4eb285cfe185e1ab9c212aab\n",
      "  Stored in directory: C:\\Users\\siddhi Golatkar\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-k0wu4i_n\\wheels\\43\\1d\\c1\\a0af68d0648debf57f875e9dda56bbac35cfc27bfa187ffc46\n",
      "Successfully built en-core-web-md\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-2.3.1\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import SpaCy Module and Load the Language Model\n",
    "\n",
    "*We are loading Medium model as Small model doesn't contain word vectors that we will use later. If you are not using word vectors, you can load Small model as well.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T09:32:19.329906Z",
     "start_time": "2019-11-16T09:31:34.429604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the language model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Create SpaCy Object\n",
    "doc = nlp(\"Hello World\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Basic steps in text pre-processing are :\n",
    "\n",
    "- Tokenisation \n",
    "- Stop Words removal\n",
    "- Matcher and PhraseMatcher\n",
    "- Lemmatization\n",
    "- Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding SpaCy Objects\n",
    "\n",
    "#### I. `nlp` Object\n",
    "\n",
    "When we load the SpaCy model, it creates the SpaCy object. We define it with the variable name `nlp`. \n",
    "\n",
    "This object contains the language specific vocabulary, model weights and processing pipeline like tokenisation rules, stop words, POS rules etc.\n",
    "\n",
    "![](nlp.png)\n",
    "\n",
    "Look for pipeline component names using `pipe_names` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T09:32:26.814413Z",
     "start_time": "2019-11-16T09:32:26.798664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x2263311f2c8>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x22632ffeb88>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x22632ffec48>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(nlp.pipe_names)\n",
    "\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the description of all these from the Spacy document.\n",
    "# Text - Is for the text in the document.\n",
    "#Lemma: The base form of the word.\n",
    "#POS: The simple part-of-speech tag.\n",
    "#Tag: The detailed part-of-speech tag.\n",
    "#Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "#Shape: The word shape – capitalization, punctuation, digits.\n",
    "#is alpha: Is the token an alpha character?\n",
    "#is stop: Is the token part of a stop list, i.e. the most common words of the language?\n",
    "# Please find this link for \"https://spacy.io/api/annotation#pos-tagging\" details about all POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us try to understand all these different pipe line elements.\n",
    "#doc = nlp(\"We are going to learn about how to process text in today's lab\")\n",
    "doc=nlp(\"Virat Kohli becomes FASTEST Indian cricketer to score 20000 international runs. can he do the same for 30000 runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virat Virat PROPN NNP compound\n",
      "Kohli Kohli PROPN NNP nsubj\n",
      "becomes become VERB VBZ ROOT\n",
      "FASTEST fastest ADJ JJ amod\n",
      "Indian indian ADJ JJ amod\n",
      "cricketer cricketer NOUN NN attr\n",
      "to to PART TO aux\n",
      "score score VERB VB advcl\n",
      "20000 20000 NUM CD nummod\n",
      "international international ADJ JJ amod\n",
      "runs run NOUN NNS dobj\n",
      ". . PUNCT . punct\n",
      "can can VERB MD aux\n",
      "he -PRON- PRON PRP nsubj\n",
      "do do AUX VB ROOT\n",
      "the the DET DT det\n",
      "same same ADJ JJ dobj\n",
      "for for ADP IN prep\n",
      "30000 30000 NUM CD nummod\n",
      "runs run NOUN NNS pobj\n"
     ]
    }
   ],
   "source": [
    "text=[]\n",
    "lemma=[]\n",
    "pos=[]\n",
    "tag=[]\n",
    "dep=[]\n",
    "shape=[]\n",
    "alpha=[]\n",
    "stop=[]\n",
    "ner_text=[]\n",
    "ner_label=[]\n",
    "for token in doc:\n",
    "    text.append(token.text)\n",
    "    lemma.append(token.lemma_)\n",
    "    pos.append(token.pos_)\n",
    "    tag.append(token.tag_)\n",
    "    dep.append(token.dep_)\n",
    "    shape.append(token.shape_)\n",
    "    alpha.append(token.is_alpha)\n",
    "    stop.append(token.is_stop)\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_)\n",
    "    #       token.shape_, token.is_alpha, token.is_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(list(zip(text,lemma,pos,tag,dep,shape,alpha,stop))\n",
    "                  ,columns=['text','lemma','pos','tag','dep','shape','alpha','stop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>dep</th>\n",
       "      <th>shape</th>\n",
       "      <th>alpha</th>\n",
       "      <th>stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Virat</td>\n",
       "      <td>Virat</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kohli</td>\n",
       "      <td>Kohli</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>becomes</td>\n",
       "      <td>become</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FASTEST</td>\n",
       "      <td>fastest</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indian</td>\n",
       "      <td>indian</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cricketer</td>\n",
       "      <td>cricketer</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>attr</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "      <td>TO</td>\n",
       "      <td>aux</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>score</td>\n",
       "      <td>score</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VB</td>\n",
       "      <td>advcl</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20000</td>\n",
       "      <td>20000</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>nummod</td>\n",
       "      <td>dddd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>international</td>\n",
       "      <td>international</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>runs</td>\n",
       "      <td>run</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>dobj</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>can</td>\n",
       "      <td>can</td>\n",
       "      <td>VERB</td>\n",
       "      <td>MD</td>\n",
       "      <td>aux</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>he</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>do</td>\n",
       "      <td>do</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>same</td>\n",
       "      <td>same</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>dobj</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30000</td>\n",
       "      <td>30000</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>nummod</td>\n",
       "      <td>dddd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>runs</td>\n",
       "      <td>run</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>pobj</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             text          lemma    pos  tag       dep  shape  alpha   stop\n",
       "0           Virat          Virat  PROPN  NNP  compound  Xxxxx   True  False\n",
       "1           Kohli          Kohli  PROPN  NNP     nsubj  Xxxxx   True  False\n",
       "2         becomes         become   VERB  VBZ      ROOT   xxxx   True   True\n",
       "3         FASTEST        fastest    ADJ   JJ      amod   XXXX   True  False\n",
       "4          Indian         indian    ADJ   JJ      amod  Xxxxx   True  False\n",
       "5       cricketer      cricketer   NOUN   NN      attr   xxxx   True  False\n",
       "6              to             to   PART   TO       aux     xx   True   True\n",
       "7           score          score   VERB   VB     advcl   xxxx   True  False\n",
       "8           20000          20000    NUM   CD    nummod   dddd  False  False\n",
       "9   international  international    ADJ   JJ      amod   xxxx   True  False\n",
       "10           runs            run   NOUN  NNS      dobj   xxxx   True  False\n",
       "11              .              .  PUNCT    .     punct      .  False  False\n",
       "12            can            can   VERB   MD       aux    xxx   True   True\n",
       "13             he         -PRON-   PRON  PRP     nsubj     xx   True   True\n",
       "14             do             do    AUX   VB      ROOT     xx   True   True\n",
       "15            the            the    DET   DT       det    xxx   True   True\n",
       "16           same           same    ADJ   JJ      dobj   xxxx   True   True\n",
       "17            for            for    ADP   IN      prep    xxx   True   True\n",
       "18          30000          30000    NUM   CD    nummod   dddd  False  False\n",
       "19           runs            run   NOUN  NNS      pobj   xxxx   True  False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we process some text with `nlp` object, it creates a doc object, short for document.\n",
    "\n",
    "#### II. `Doc` Object\n",
    "\n",
    "![](doc.png)\n",
    "\n",
    "Token objects represent the word tokens in the document. To get a token at a specific position, simply index the Doc object like any python object.\n",
    "\n",
    "#### III. `Span` Object\n",
    "\n",
    "![](span.png)\n",
    "\n",
    "A Span object is a slice of the document consisting of one or more tokens. Again to view a span, simply index with start and end position seperated by : like any python object.\n",
    "\n",
    "## Tokenisation\n",
    "\n",
    "#### I. Word Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T09:32:31.851645Z",
     "start_time": "2019-11-16T09:32:31.821911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc Object   :  Have a Good Day!\n",
      "Token Object :  a\n",
      "Token Object :  !\n",
      "Span Object  :  a Good Day\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Have a Good Day!\")\n",
    "\n",
    "# Print the document text\n",
    "print(\"Doc Object   : \", doc.text)\n",
    "\n",
    "# Get the token text using .text attribute\n",
    "print(\"Token Object : \", doc[1].text) \n",
    "print(\"Token Object : \", doc[-1].text) # punctuation is also a token\n",
    "\n",
    "# Take a span of tokens\n",
    "span = doc[1:4]\n",
    "\n",
    "# Get the span text using .text attribute\n",
    "print(\"Span Object  : \", span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.lang_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case all you want to do is tokenise and don't want to initialise rest of the pipeline. There are two options.\n",
    "\n",
    "Let's compare them using the pos_ attribute on the token and check the output with only tokeniser and with complete pipeline.\n",
    "\n",
    "- Default Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T09:35:08.589600Z",
     "start_time": "2019-11-16T09:35:08.565402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n",
      "PUNCT\n",
      "NN\n"
     ]
    }
   ],
   "source": [
    "# Process the text \n",
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "print(doc.text)\n",
    "print(doc[2].pos_)\n",
    "print(doc[1].tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Doc object Only\n",
    "\n",
    "`make_doc( )` function creates a doc object with only tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T09:35:10.849796Z",
     "start_time": "2019-11-16T09:35:10.843538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process the text \n",
    "doc = nlp.make_doc(\"Hello world!\")\n",
    "\n",
    "print(doc.text)\n",
    "print(doc[2].pos_)\n",
    "print(doc[1].tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokeniser object ( Temporarily )\n",
    "\n",
    "Disable remaining pipeline in with statement. This doc is not available outside with statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T09:35:13.379685Z",
     "start_time": "2019-11-16T09:35:13.366580Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Disable tagger and parser\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    \n",
    "    # Process the text \n",
    "    doc = nlp(\"Hello world!\")\n",
    "    print(doc[1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Sentence Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T09:35:15.047576Z",
     "start_time": "2019-11-16T09:35:15.022115Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Have a Good Day!', 'Same to you']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Have a Good Day! Same to you\")\n",
    "\n",
    "[w.text for w in doc.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Pipeline Component\n",
    "\n",
    "Let's see how to add a custom component to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T09:41:12.257991Z",
     "start_time": "2019-11-16T09:41:11.381588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'size_info']\n",
      "This doc has 4 tokens.\n",
      "This is a small review\n",
      "This doc has 11 tokens.\n",
      "This is a long review\n"
     ]
    }
   ],
   "source": [
    "nlp1 = spacy.load('en_core_web_md')\n",
    "\n",
    "\n",
    "def add_comp1(doc):\n",
    "    \n",
    "    print(\"This doc has {} tokens.\".format(len(doc)))\n",
    "    \n",
    "    if len(doc) < 10 :\n",
    "        print(\"This is a small review\")\n",
    "    elif len(doc) > 10 :\n",
    "        print(\"This is a long review\")\n",
    "        \n",
    "    return doc\n",
    "\n",
    "nlp1.add_pipe(add_comp1, name = \"size_info\", last = True)\n",
    "\n",
    "print(nlp1.pipe_names)  \n",
    "\n",
    "doc = nlp1(\"The moview was good\")\n",
    "\n",
    "doc = nlp1(\"I loved the movie. It was a great experience!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP WORDS\n",
    "\n",
    "Words that occur very frequently in the documents that they don't add any meaning or value are called stop words. It best to remove these words that are useless and consume a lot of resources.\n",
    "\n",
    "##### Remember : Stop words are language & domain dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T09:56:36.701984Z",
     "start_time": "2019-11-16T09:56:36.692372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['everyone', 'seems', 'rather', 'whereupon', 'was', 'cannot', 'six', 'might', 'and', 'own', 'whose', 'during', 'around', 'very', 'myself', 'anything', 'to', 'whither', 'on', '’re', 'last', 'within', 'various', 'already', 'while', 'less', 'how', 'whereby', 'many', 'amongst', 'do', 'namely', \"'ve\", 'whereas', 'regarding', 'top', 'he', 'together', 'first', 'will', 'into', 'once', 'throughout', 'should', 'becomes', 'become', 'herein', 'as', 'behind', 'down', 'she', 'ten', 'themselves', 'show', '’ll', 'well', 'either', '‘ve', \"'ll\", 'hence', 'due', 'another', 'perhaps', 'quite', 'wherever', 'me', 'side', 'yet', 'have', 'although', 'third', 'him', 'for', 'i', 'four', 'did', 'could', 'himself', 'sixty', 're', 'everything', 'before', 'out', \"'m\", 'from', 'the', 'where', \"'s\", 'more', 'who', 'ever', 'formerly', 'under', 'front', 'same', 'of', 'also', 'somehow', '’ve', 'had', 'anyway', 'other', 'been', 'when', 'toward', 'enough', 'keep', 'seeming', 'why', 'next', 'but', 'now', 'with', 'former', 'sometimes', 'thus', 'those', 'too', 'much', 'sometime', 'further', 'mostly', '’d', 'we', 'off', 'doing', '‘s', 'becoming', 'ourselves', 'make', 'go', 'nevertheless', 'may', 'nor', 'this', '‘d', 'her', 'does', 'done', 'above', 'eleven', 'never', 'twenty', 'whenever', 'amount', 'no', 'if', 'bottom', 'hereafter', 'it', 'say', 'anyone', 'yourselves', 'has', 'thereupon', 'serious', 'others', 'nine', 'each', 'per', 'elsewhere', 'beforehand', 'yourself', 'put', 'must', 'anywhere', 'being', \"'d\", 'beside', 'nothing', 'however', 'are', 'am', 'since', 'whence', 'became', 'alone', 'you', 'afterwards', 'by', 'five', 'whole', 'whoever', 'latter', 'yours', 'nobody', 'most', 'our', 'that', 'wherein', 'these', 'noone', 'there', 'nowhere', 'again', 'thru', 'an', \"n't\", 'below', 'between', 'what', 'everywhere', 'hundred', 'or', 'about', 'least', 'whereafter', 'except', '’m', 'whatever', 'n‘t', 'almost', 'move', 'across', 'often', 'through', 'seemed', 'until', 'therein', 'anyhow', 'be', 'used', 'upon', 'which', 'something', 'using', 'here', 'full', 'over', 'because', 'meanwhile', 'them', 'some', 'hereby', 'hereupon', 'mine', 'made', 'thence', '’s', 'indeed', 'few', 'their', 'such', 'somewhere', 'latterly', 'name', 'not', 'itself', 'they', 'all', 'hers', 'really', 'see', 'towards', 'a', 'along', 'every', 'then', 'besides', 'still', 'even', \"'re\", 'can', 'else', 'eight', 'any', 'n’t', 'several', 'only', 'three', 'up', 'its', 'get', '‘re', 'so', 'without', 'is', 'take', 'call', 'thereafter', 'two', 'none', 'though', 'after', 'unless', 'twelve', 'part', 'empty', '‘ll', '‘m', 'his', 'beyond', 'fifteen', 'moreover', 'onto', 'my', 'against', 'whether', 'at', 'were', 'herself', 'us', 'your', 'than', 'in', 'one', 'fifty', 'would', 'give', 'neither', 'ours', 'via', 'ca', 'whom', 'back', 'forty', 'thereby', 'therefore', 'always', 'please', 'both', 'just', 'seem', 'otherwise', 'among', 'someone']\n",
      "['Hi', 'Bye', 'everyone', 'seems', 'rather', 'whereupon', 'was', 'cannot', 'six', 'might', 'and', 'own', 'whose', 'during', 'around', 'very', 'myself', 'anything', 'to', 'whither', 'on', '’re', 'last', 'within', 'various', 'already', 'while', 'less', 'how', 'whereby', 'many', 'amongst', 'do', 'namely', \"'ve\", 'whereas', 'regarding', 'top', 'he', 'together', 'first', 'will', 'into', 'once', 'throughout', 'should', 'becomes', 'become', 'herein', 'as', 'behind', 'down', 'she', 'ten', 'themselves', 'show', '’ll', 'well', 'either', '‘ve', \"'ll\", 'hence', 'due', 'another', 'perhaps', 'quite', 'wherever', 'me', 'side', 'yet', 'have', 'although', 'third', 'him', 'for', 'i', 'four', 'did', 'could', 'himself', 'sixty', 're', 'everything', 'before', 'out', \"'m\", 'from', 'the', 'where', \"'s\", 'more', 'who', 'ever', 'formerly', 'under', 'front', 'same', 'of', 'also', 'somehow', '’ve', 'had', 'anyway', 'other', 'been', 'when', 'toward', 'enough', 'keep', 'seeming', 'why', 'next', 'but', 'now', 'with', 'former', 'sometimes', 'thus', 'those', 'too', 'much', 'sometime', 'further', 'mostly', '’d', 'we', 'off', 'doing', '‘s', 'becoming', 'ourselves', 'make', 'go', 'nevertheless', 'may', 'nor', 'this', '‘d', 'her', 'does', 'done', 'above', 'eleven', 'never', 'twenty', 'whenever', 'amount', 'no', 'if', 'bottom', 'hereafter', 'it', 'say', 'anyone', 'yourselves', 'has', 'thereupon', 'serious', 'others', 'nine', 'each', 'per', 'elsewhere', 'beforehand', 'yourself', 'put', 'must', 'anywhere', 'being', \"'d\", 'beside', 'nothing', 'however', 'are', 'am', 'since', 'whence', 'became', 'alone', 'you', 'afterwards', 'by', 'five', 'whole', 'whoever', 'latter', 'yours', 'nobody', 'most', 'our', 'that', 'wherein', 'these', 'noone', 'there', 'nowhere', 'again', 'thru', 'an', \"n't\", 'below', 'between', 'what', 'everywhere', 'hundred', 'or', 'about', 'least', 'whereafter', 'except', '’m', 'whatever', 'n‘t', 'almost', 'move', 'across', 'often', 'through', 'seemed', 'until', 'therein', 'anyhow', 'be', 'used', 'upon', 'which', 'something', 'using', 'here', 'full', 'over', 'because', 'meanwhile', 'them', 'some', 'hereby', 'hereupon', 'mine', 'made', 'thence', '’s', 'indeed', 'few', 'their', 'such', 'somewhere', 'latterly', 'name', 'not', 'itself', 'they', 'all', 'hers', 'really', 'see', 'towards', 'a', 'along', 'every', 'then', 'besides', 'still', 'even', \"'re\", 'can', 'else', 'eight', 'any', 'n’t', 'several', 'only', 'three', 'up', 'its', 'get', '‘re', 'so', 'without', 'is', 'take', 'call', 'thereafter', 'two', 'none', 'though', 'after', 'unless', 'twelve', 'part', 'empty', '‘ll', '‘m', 'his', 'beyond', 'fifteen', 'moreover', 'onto', 'my', 'against', 'whether', 'at', 'were', 'herself', 'us', 'your', 'than', 'in', 'one', 'fifty', 'would', 'give', 'neither', 'ours', 'via', 'ca', 'whom', 'back', 'forty', 'thereby', 'therefore', 'always', 'please', 'both', 'just', 'seem', 'otherwise', 'among', 'someone']\n",
      "['everyone', 'are', 'am', 'since', 'whence', 'seems', 'became', 'rather', 'whereupon', 'however', 'alone', 'you', 'afterwards', 'cannot', 'six', 'might', 'by', 'five', 'and', 'own', 'whose', 'whole', 'whoever', 'latter', 'during', 'yours', 'around', 'very', 'myself', 'anything', 'to', 'nobody', 'whither', 'most', 'on', 'our', '’re', 'that', 'last', 'within', 'wherein', 'these', 'various', 'noone', 'already', 'while', 'less', 'there', 'how', 'whereby', 'nowhere', 'many', 'again', 'thru', 'amongst', 'do', 'an', \"n't\", 'below', 'namely', 'between', \"'ve\", 'what', 'everywhere', 'hundred', 'or', 'whereas', 'about', 'least', 'regarding', 'whereafter', 'top', 'he', '’m', 'together', 'whatever', 'n‘t', 'almost', 'move', 'across', 'often', 'through', 'first', 'seemed', 'will', 'until', 'into', 'once', 'throughout', 'should', 'therein', 'anyhow', 'becomes', 'be', 'used', 'upon', 'become', 'which', 'herein', 'as', 'behind', 'down', 'she', 'ten', 'something', 'using', 'themselves', 'here', 'full', 'show', 'over', 'because', 'meanwhile', 'them', 'some', '’ll', 'hereby', 'well', 'either', '‘ve', 'hereupon', 'mine', \"'ll\", 'hence', 'made', 'thence', '’s', 'due', 'indeed', 'another', 'perhaps', 'quite', 'few', 'wherever', 'me', 'side', 'their', 'yet', 'have', 'although', 'third', 'him', 'such', 'somewhere', 'for', 'latterly', 'i', 'name', 'not', 'itself', 'four', 'did', 'could', 'himself', 'they', 'all', 'hers', 'really', 'sixty', 're', 'see', 'towards', 'everything', 'a', 'before', 'out', 'along', 'every', 'then', 'besides', \"'m\", 'from', 'still', 'even', 'the', 'where', \"'re\", \"'s\", 'more', 'who', 'ever', 'formerly', 'under', 'can', 'front', 'else', 'eight', 'any', 'same', 'of', 'also', 'somehow', '’ve', 'had', 'n’t', 'anyway', 'other', 'been', 'several', 'when', 'toward', 'only', 'three', 'up', 'enough', 'keep', 'its', 'get', 'seeming', 'why', 'next', '‘re', 'so', 'without', 'but', 'now', 'is', 'take', 'with', 'call', 'former', 'sometimes', 'thereafter', 'thus', 'two', 'those', 'too', 'much', 'sometime', 'further', 'none', 'mostly', '’d', 'we', 'off', 'though', 'doing', 'after', 'unless', 'twelve', 'part', 'empty', '‘s', '‘ll', 'becoming', 'ourselves', '‘m', 'his', 'beyond', 'make', 'fifteen', 'go', 'nevertheless', 'may', 'moreover', 'nor', 'this', '‘d', 'onto', 'my', 'against', 'whether', 'her', 'at', 'does', 'were', 'herself', 'us', 'done', 'above', 'eleven', 'never', 'twenty', 'whenever', 'amount', 'your', 'than', 'no', 'if', 'in', 'one', 'someone', 'bottom', 'hereafter', 'fifty', 'it', 'say', 'would', 'give', 'neither', 'ours', 'via', 'anyone', 'yourselves', 'ca', 'whom', 'has', 'thereupon', 'back', 'serious', 'forty', 'thereby', 'therefore', 'always', 'please', 'others', 'nine', 'both', 'each', 'per', 'elsewhere', 'beforehand', 'yourself', 'just', 'seem', 'put', 'must', 'anywhere', 'being', \"'d\", 'otherwise', 'among', 'beside', 'nothing']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# View the default stop words list\n",
    "print(list(STOP_WORDS))#[:10])\n",
    "\n",
    "\n",
    "# Add some stop words to default list\n",
    "stopwords = ['Hi', 'Bye'] + list(STOP_WORDS)\n",
    "print(stopwords)#[:10])\n",
    "\n",
    "# Remove some stop words to default list\n",
    "stopwords = set(STOP_WORDS) - {'except', 'was'}\n",
    "print(list(stopwords))#[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Attributes\n",
    "\n",
    "Attributes that don't hold any contextual information are called lexical attributes. \n",
    "Let's explore other available token attributes :\n",
    "\n",
    "- i - index\n",
    "- text - token text\n",
    "- is_alpha - alphanumeric character (True/False)\n",
    "- is_punct - punctuation (True/False)\n",
    "- like_num - alphanumeric character (True/False)\n",
    "\n",
    "refer to https://github.com/explosion/spaCy/issues/1439 for all available lexical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T09:56:48.289814Z",
     "start_time": "2019-11-16T09:56:48.256407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Text:     ['I', 'was', 'stuck', 'in', 'traffic', 'for', 'two', 'hours', 'and', 'reached', 'home', '@', '8']\n",
      "is_alpha: [True, True, True, True, True, True, True, True, True, True, True, False, False]\n",
      "is_punct: [False, False, False, False, False, False, False, False, False, False, False, True, False]\n",
      "like_num: [False, False, False, False, False, False, True, False, False, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I was stuck in traffic for two hours and reached home @ 8\")\n",
    "\n",
    "print('Index:   ', [token.i for token in doc])\n",
    "print('Text:    ', [token.text for token in doc])\n",
    "print('is_alpha:', [token.is_alpha for token in doc])\n",
    "print('is_punct:', [token.is_punct for token in doc])\n",
    "print('like_num:', [token.like_num for token in doc]) # observe both 'two' and '8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmas are root form of a word. It is helpful to reduce the bag of words by using the same root word for all similar kind of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T10:13:04.287934Z",
     "start_time": "2019-11-16T10:13:04.256154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Unfortunately', 'unfortunately'), ('all', 'all'), ('my', '-PRON-'), ('friends', 'friend'), ('are', 'be'), ('somewhere', 'somewhere'), ('in', 'in'), ('unknown', 'unknown'), ('land', 'land')]\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Unfortunately all my friends are somewhere in unknown land\")\n",
    "# Print the text and the predicted tags\n",
    "print([(w.text, w.lemma_) for w in doc]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Models\n",
    "\n",
    "Now, let us look at some context based attributes. All the information to make these predictions is also loaded with the model. These models are trained on large datasets of labeled example texts. \n",
    "\n",
    "#### I. Part of Speech\n",
    "\n",
    "- pos_ - Part of Speech\n",
    "\n",
    "POS means labeling words in a sentence as nouns, adjectives, verbs, tense etc. This is particularly helpful for identifying homophones in speech to text analysis. _*eg. If you accidentally drank a bottle of fabric dye, you might die.*_ **[ GOOD TO KNOW 😀 ]**\n",
    "\n",
    "\n",
    "refer to https://spacy.io/api/annotation#pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T10:17:11.036397Z",
     "start_time": "2019-11-16T10:17:10.988578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRON'), ('am', 'AUX'), ('20', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), ('😀', 'PUNCT')]\n",
      "[('pen', 'VERB'), ('a', 'DET'), ('letter', 'NOUN'), ('to', 'ADP'), ('your', 'DET'), ('pen', 'NOUN'), ('pal', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"I am 20 years old 😀\")\n",
    "\n",
    "# Print the text and the predicted tags\n",
    "print([(w.text, w.pos_) for w in doc])\n",
    "\n",
    "\n",
    "doc = nlp(\"pen a letter to your pen pal\")\n",
    "\n",
    "print([(w.text, w.pos_) for w in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use spacy.explain( [tag] ) function to find the meaning of the different tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T10:17:12.629599Z",
     "start_time": "2019-11-16T10:17:12.620504Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pronoun\n",
      "numeral\n",
      "verb\n",
      "noun\n",
      "adjective\n",
      "adposition\n",
      "determiner\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('PRON'))\n",
    "print(spacy.explain('NUM'))\n",
    "print(spacy.explain('VERB'))\n",
    "print(spacy.explain('NOUN'))\n",
    "print(spacy.explain('ADJ'))\n",
    "print(spacy.explain('ADP'))\n",
    "print(spacy.explain('DET'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determiners** are one of the ingredients of noun phrases that is, determining exactly which of several possible alternative objects in the world is referred to by a noun phrase\n",
    "\n",
    "**Adpositional** phrases contain an adposition (preposition, postposition, or circumposition) as head and usually a complement such as a noun phrase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dep_ - syntatic dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T10:20:17.891153Z",
     "start_time": "2019-11-16T10:20:17.860967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'nsubj'), ('am', 'ROOT'), ('20', 'nummod'), ('years', 'npadvmod'), ('old', 'acomp')]\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"I am 20 years old\")\n",
    "\n",
    "# Print the text and the predicted tags\n",
    "print([(w.text, w.dep_) for w in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T10:20:17.904226Z",
     "start_time": "2019-11-16T10:20:17.895732Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nominal subject\n",
      "None\n",
      "numeric modifier\n",
      "noun phrase as adverbial modifier\n",
      "adjectival complement\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('nsubj'))\n",
    "print(spacy.explain('ROOT'))\n",
    "print(spacy.explain('nummod'))\n",
    "print(spacy.explain('npadvmod'))\n",
    "print(spacy.explain('acomp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise using displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T10:17:54.600947Z",
     "start_time": "2019-11-16T10:17:54.587630Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"3323a15b28384ff9bbbd967e6783a396-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">am</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">20</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">years</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">old</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3323a15b28384ff9bbbd967e6783a396-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3323a15b28384ff9bbbd967e6783a396-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3323a15b28384ff9bbbd967e6783a396-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3323a15b28384ff9bbbd967e6783a396-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3323a15b28384ff9bbbd967e6783a396-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3323a15b28384ff9bbbd967e6783a396-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3323a15b28384ff9bbbd967e6783a396-0-3\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3323a15b28384ff9bbbd967e6783a396-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy # for visualisation\n",
    "\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T10:20:17.856019Z",
     "start_time": "2019-11-16T10:17:58.404349Z"
    }
   },
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.serve(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Named entities are \"real world objects\" that are assigned a name, such as people, places, things, locations, currencies, and more.\n",
    "\n",
    "Named entities can be accessed by using `doc.ents` which returns and iterator of span objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T10:20:31.528554Z",
     "start_time": "2019-11-16T10:20:31.493038Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virat Kohli PERSON 0 11\n",
      "India GPE 13 18\n",
      "Sachin PERSON 30 36\n",
      "Lara PERSON 41 45\n",
      "20,000 CARDINAL 66 72\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Virat Kohli (India) surpasses Sachin and Lara, becomes fastest to 20,000 international runs.\")\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_, ent.start_char, ent.end_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T10:22:51.575159Z",
     "start_time": "2019-11-16T10:22:51.569138Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerals that do not fall under another type \n",
      "\n",
      "\"first\", \"second\", etc. \n",
      "\n",
      "Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('CARDINAL'), '\\n')\n",
    "\n",
    "print(spacy.explain('ORDINAL'), '\\n')\n",
    "\n",
    "print(spacy.explain('GPE'))\n",
    "\n",
    "#GPE  - Geo political entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T10:23:29.842913Z",
     "start_time": "2019-11-16T10:23:29.835961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Virat Kohli\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    India\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ") surpasses \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sachin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Lara\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", becomes fastest to \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    20,000\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " international runs.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Matching\n",
    "\n",
    "#### I. Token Matcher\n",
    "\n",
    "Similar to regular expressions, `Matcher` module in SpaCy allow pattern matching for Doc and Token objects.\n",
    "\n",
    "A pattern object is a list of dictionaries that can be added to the matcher object using add( ) function.\n",
    "\n",
    "- The first argument is a unique name for the pattern. \n",
    "- The second argument is an optional callback. We don't need it, so we set it to None. \n",
    "- The third argument is the pattern.\n",
    "\n",
    "Matcher returns a list of tuples each consisting \n",
    "\n",
    "- hash value of pattern name\n",
    "- start index\n",
    "- end index\n",
    "\n",
    "Try the make a token optional use `OP` key\n",
    "\n",
    "- `!` - match 0 times (negate)\n",
    "- `?` - match 0 or 1 times (optional)\n",
    "- `+` - match 1 or more times \n",
    "- `*` - match 0 or more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T10:25:48.779052Z",
     "start_time": "2019-11-16T10:25:48.762291Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T10:37:47.731195Z",
     "start_time": "2019-11-16T10:37:47.713706Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. match text as such and case-insensitive\n",
    "pattern_1 = [{'TEXT': 'Viratkohli'}, {'LOWER': 'kohli'}]\n",
    "\n",
    "# 2. match digit\n",
    "pattern_2 = [{'IS_DIGIT': True}]\n",
    "\n",
    "# 3. match lemma and part of speech\n",
    "# the \"?\" operator makes the fast lemma token optional\n",
    "pattern_3 = [{'LEMMA': 'fast', 'OP': '?'}, {'POS': 'ADJ'}]\n",
    "\n",
    "# 4. match lemma and part of speech\n",
    "pattern_4 = [{'IS_UPPER': True}]\n",
    "#pattern_4 = [{'TEXT': 'ViratKohli'}]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('PATTERN_1', None, pattern_1)\n",
    "matcher.add('PATTERN_2', None, pattern_2)\n",
    "matcher.add('PATTERN_3', None, pattern_3)\n",
    "matcher.add('PATTERN_4', None, pattern_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T10:37:48.605059Z",
     "start_time": "2019-11-16T10:37:48.567337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastest\n",
      "fastest Indian\n",
      "Indian\n",
      "international\n",
      "same\n"
     ]
    }
   ],
   "source": [
    "# Process some text\n",
    "doc = nlp(\"Virat Kohli becomes fastest Indian cricketer to score 20000 international runs. can he do the same for 30000 runs\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    if(nlp.vocab.strings[match_id] == 'PATTERN_3'):\n",
    "        \n",
    "        #print(nlp.vocab.strings[match_id])\n",
    "        matched_span = doc[start:end]\n",
    "        print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Phrase Matcher\n",
    "\n",
    "In case you want to match a span or phrase, use `PhraseMatcher` and create doc object as patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T10:38:01.985425Z",
     "start_time": "2019-11-16T10:38:01.978859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T10:38:02.575031Z",
     "start_time": "2019-11-16T10:38:02.533242Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. match text as such and case-insensitive\n",
    "pattern_1 = nlp('Virat Kohli')\n",
    "\n",
    "# 2. match text as such and case-insensitive\n",
    "pattern_2 = nlp('20000')\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('NAME', None, pattern_1)\n",
    "matcher.add('RUNS', None, pattern_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T10:38:05.429036Z",
     "start_time": "2019-11-16T10:38:05.399152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virat Kohli\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "# Process some text\n",
    "doc = nlp(\"Virat Kohli becomes fastest Indian cricketer to score 20000 international runs.\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    \n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity\n",
    "\n",
    "When we load the language model, it also loads the 300-dimensional vector representation for the words. The vector representation has been computed using the Word2Vec algorithm on large Web text.\n",
    "\n",
    "*You will learn more about Word2Vec in Deep Learning Module.*\n",
    "\n",
    "*Its important to note that the small model doesn't contatin word vectors. That is also the reason it loads faster.*\n",
    "\n",
    "SpaCy computes a similarity score between 0-1 between these vectors, at 3 levels \n",
    "- Doc,\n",
    "- Span, and\n",
    "- Token, \n",
    "\n",
    "using the similarity( ) function. By default, it uses cosine similarity.\n",
    "\n",
    "*Similarity is averaged for Doc and Span.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:28:51.003118Z",
     "start_time": "2019-11-16T11:28:50.962146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.9832e-01  1.7471e-01  7.0592e-02  5.1045e-01  2.3978e-01  7.5919e-02\n",
      "  2.3031e-01 -6.1956e-01  2.1394e-01 -1.1881e+00 -1.7273e-01 -3.8365e-01\n",
      " -8.6379e-01  2.4508e-02  9.3082e-02  4.1044e-01 -2.0216e-01 -1.3796e+00\n",
      "  4.1507e-01  5.9532e-01  1.7056e-01  5.0616e-01  4.3366e-01  4.9059e-01\n",
      " -3.0667e-01  4.5887e-01  1.7847e-02  7.8978e-01 -3.6883e-01  5.2176e-01\n",
      "  3.1528e-01 -9.4568e-03 -1.0214e-01  3.8308e-01 -7.4116e-01  2.5485e-01\n",
      " -3.8609e-01 -1.3838e-01  9.9488e-02 -4.1722e-01  4.0797e-01 -5.3534e-01\n",
      " -3.1326e-01 -1.7172e-01 -4.0924e-01  3.3197e-01 -1.9134e-01  1.1649e-02\n",
      "  1.6144e-01 -2.4808e-01  4.0267e-01  2.9613e-01  6.2917e-02 -2.6367e-01\n",
      " -4.7752e-01  1.1263e-02 -1.2182e-01  3.3558e-01 -4.6654e-01 -1.8428e-01\n",
      "  2.5530e-02 -1.4487e-01 -4.4719e-02 -2.5467e-02 -7.3004e-02  5.4738e-01\n",
      "  7.8649e-02  2.9088e-01  1.5538e-01 -4.4414e-01  2.1993e-03 -2.1081e-01\n",
      "  1.5998e-01 -7.3798e-01  2.2719e-01 -7.2435e-01 -5.0498e-01  4.7991e-02\n",
      "  5.4960e-01 -2.2687e-01  4.4384e-01 -1.9989e-01  2.6162e-01  7.0157e-01\n",
      " -1.0815e-01 -1.9819e-01  1.1684e+00 -3.4274e-01 -1.0901e-01 -4.8786e-01\n",
      " -3.4872e-01 -9.5544e-02 -6.2328e-01 -2.7593e-01  5.7172e-01  3.2734e-01\n",
      "  9.3017e-02 -4.1861e-01 -5.2112e-01 -3.6743e-01  2.5636e-01 -7.5818e-01\n",
      " -3.4817e-01 -1.9175e-01  6.1267e-03 -1.3618e+00 -7.2516e-01 -1.9872e-01\n",
      "  3.8641e-01  1.1044e-01 -1.8010e-01 -1.8099e-01  7.3046e-01 -2.0181e-01\n",
      "  4.6358e-01  1.0507e-01  6.3685e-01  6.9819e-01 -1.6141e-01 -6.1782e-01\n",
      " -3.7200e-01  3.4649e-01 -9.1107e-03  3.6265e-01  4.7032e-01  5.5006e-01\n",
      "  5.8578e-02 -6.3974e-02 -1.7195e-01 -8.2257e-02  3.8306e-02 -5.1057e-01\n",
      "  4.6162e-01 -4.9327e-01 -2.5698e-01  1.2622e-01 -4.2338e-01  1.5756e-01\n",
      "  2.3992e-01 -6.0445e-02 -2.0881e-01 -2.2697e-01 -3.5150e-01  9.8038e-01\n",
      " -3.1027e-01 -1.9599e-01 -2.3820e-02  2.4809e-01 -1.5286e-02  5.1643e-01\n",
      "  5.1180e-02 -2.4916e-01 -3.2044e-01  8.8840e-01  3.8730e-01  1.7669e-02\n",
      " -3.0996e-01 -1.8373e-01 -4.4646e-01  5.6889e-01  8.9490e-01 -3.8131e-01\n",
      "  1.0496e-01 -4.7026e-01  4.1761e-01 -1.3598e-01 -2.2293e-01 -4.3728e-01\n",
      " -7.4945e-01  4.4248e-02 -4.3045e-01 -4.8376e-01 -1.6265e-01  1.0318e-03\n",
      " -3.2195e-01 -9.4401e-01  3.5489e-01  6.2188e-02 -4.2378e-01 -3.7526e-01\n",
      " -9.3049e-01 -2.9614e-01 -1.4092e-01  1.8747e-01  1.8430e-01  3.7539e-01\n",
      "  2.1765e-02 -5.5020e-02  3.6201e-01 -5.1356e-02 -6.9036e-01 -4.3650e-01\n",
      " -6.0260e-01 -4.0345e-01  2.8776e-01 -7.8038e-02 -3.6173e-01 -3.4004e-01\n",
      " -2.2281e-01  5.6848e-01  1.6719e-01  2.6765e-01  3.9929e-01 -4.3413e-01\n",
      "  2.8054e-01 -1.6771e-01  3.2949e-01  8.2134e-02  4.4907e-01 -8.9879e-02\n",
      " -3.2885e-01  9.7572e-02  1.0701e-01 -3.8733e-01 -2.0622e-01 -3.0952e-01\n",
      "  7.7995e-01  9.2140e-02  4.6749e-01 -2.3249e-01  4.9351e-02  1.4282e-01\n",
      "  3.4758e-01  1.6618e-01 -3.6362e-01  7.5526e-02 -5.8117e-01 -1.3737e-01\n",
      " -6.2803e-01 -6.9712e-01 -5.8073e-01 -1.9061e-01 -1.2428e-02  4.6013e-01\n",
      " -1.0613e+00 -3.0966e-02  6.0271e-01 -1.2220e-01  2.8821e-01  2.4344e-01\n",
      " -5.5669e-01  1.0682e+00  1.0306e-03 -4.1243e-02  1.8721e-01 -2.8439e-01\n",
      " -4.8458e-01  3.5518e-01  3.7014e-02 -3.7345e-01 -2.1644e-01  5.3836e-01\n",
      " -1.7874e-01  3.8449e-01 -4.6711e-01 -4.1724e-02  2.8184e-01  1.1547e-01\n",
      " -5.1742e-01 -1.3962e+00  3.4417e-01  5.2339e-01  1.0341e+00  5.5010e-01\n",
      " -4.0277e-01 -7.0259e-01  5.2108e-01  4.2100e-01 -3.2766e-01  1.6396e-01\n",
      " -2.8539e-01  6.7881e-01 -1.3425e-03  1.9496e-01 -2.8886e-01  3.7637e-01\n",
      "  1.3091e-01  2.7179e-01  3.0134e-01  1.9964e-01 -2.7050e-01 -5.6043e-01\n",
      " -5.4140e-01 -6.7232e-01  3.0939e-01  4.3839e-01 -3.7351e-02  1.1932e-01\n",
      "  2.0856e-02 -2.4434e-01 -2.2094e-02 -9.2085e-01 -1.0949e-01 -3.8317e-01\n",
      "  1.3824e-01  6.0450e-01 -4.5901e-01  8.0913e-02 -1.6925e-01 -2.1354e-01]\n"
     ]
    }
   ],
   "source": [
    "# Process some text\n",
    "doc = nlp(\"Virat Kohli becomes fastest Indian cricketer to score 20000 international runs.\")\n",
    "\n",
    "# Access the vector via the token.vector attribute\n",
    "print(doc[0].vector) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Token Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:28:52.165444Z",
     "start_time": "2019-11-16T11:28:52.113879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning interesting\n",
      "0.40250292\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"We are learning SpaCy\")\n",
    "doc2 = nlp(\"SpaCy is interesting\")\n",
    "\n",
    "token1 = doc1[2]\n",
    "token2 = doc2[2]\n",
    "print(token1,token2)\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Span Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:28:53.201243Z",
     "start_time": "2019-11-16T11:28:53.139541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning SpaCy ##### SpaCy is interesting\n",
      "0.42746237\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"We are learning SpaCy\")\n",
    "doc2 = nlp(\"SpaCy is interesting\")\n",
    "\n",
    "span1 = doc1[2:]\n",
    "span2 = doc2[:]\n",
    "print(span1,\"#####\",span2)\n",
    "print(span1.similarity(span2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. Doc Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:28:54.215520Z",
     "start_time": "2019-11-16T11:28:54.164154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6511438662010053\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"We are learning SpaCy\")\n",
    "doc2 = nlp(\"SpaCy is interesting\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VECTORIZATION\n",
    "\n",
    "Once the bag of words is created it needs to be encoded as integers or floating point values to be used as an input to a machine learning algorithm. This is called feature extraction (or vectorization).\n",
    "\n",
    "Let us understand Vectorization with a small example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:28:55.181289Z",
     "start_time": "2019-11-16T11:28:55.175037Z"
    }
   },
   "outputs": [],
   "source": [
    "text = ['This is the first document.', \n",
    "        'This is the second second document.', \n",
    "        'And the third one.', \n",
    "        'Is this the first document?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. CountVectorizer\n",
    "\n",
    "The CountVectorizer provides a simple way to tokenize a collection of text documents, build a vocabulary of known words and create a document- token matrix.\n",
    "\n",
    "Let's use CountVectorizer from sklearn and create an instance of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:28:57.776121Z",
     "start_time": "2019-11-16T11:28:56.130351Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create the transform\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the fit() function in order to learn a vocabulary from one or more documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:28:58.403548Z",
     "start_time": "2019-11-16T11:28:58.386454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n"
     ]
    }
   ],
   "source": [
    "# tokenize and build vocab\n",
    "count_vectorizer.fit(text)\n",
    "\n",
    "# summarize\n",
    "print(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the transform() function on one or more documents as needed to encode each as a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:28:59.735337Z",
     "start_time": "2019-11-16T11:28:59.731867Z"
    }
   },
   "outputs": [],
   "source": [
    "# encode document\n",
    "count_matrix = count_vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:29:00.611734Z",
     "start_time": "2019-11-16T11:29:00.606458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "# summarize encoded vector\n",
    "print(count_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because these vectors will contain a lot of zeros, we call them sparse. \n",
    "\n",
    "Python provides an efficient way of handling sparse vectors in the scipy.sparse package. \n",
    "\n",
    "The vectors returned from a call to transform() will be sparse vectors, and you can transform them back to numpy arrays to look and understand what is going on by calling the toarray() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:29:01.677225Z",
     "start_time": "2019-11-16T11:29:01.663558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(type(count_matrix))\n",
    "print()\n",
    "print(count_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are all the word tokens (without punctuation) in sorted order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:29:06.129278Z",
     "start_time": "2019-11-16T11:29:04.995690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  document  first  is  one  second  the  third  this\n",
       "0    0         1      1   1    0       0    1      0     1\n",
       "1    0         1      0   1    0       2    1      0     1\n",
       "2    1         0      0   0    1       0    1      1     0\n",
       "3    0         1      1   1    0       0    1      0     1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(count_vectorizer.get_feature_names())\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One can see that with CountVectorizer, all words were made lowercase by default and that the punctuation was ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-gram CountVectorizer\n",
    "\n",
    "Note that in the previous corpus, the first and the last documents have exactly the same words hence are encoded in equal vectors. \n",
    "\n",
    "We lost the information that the last document is an interrogative form. \n",
    "\n",
    "To preserve some of the local ordering information we can extract 2-grams of words in addition to the 1-grams (individual words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:29:15.763431Z",
     "start_time": "2019-11-16T11:29:15.752860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "bigram_count_vectorizer.fit_transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:29:19.149744Z",
     "start_time": "2019-11-16T11:29:19.143026Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'and the', 'document', 'first', 'first document', 'is', 'is the', 'is this', 'one', 'second', 'second document', 'second second', 'the', 'the first', 'the second', 'the third', 'third', 'third one', 'this', 'this is', 'this the']\n"
     ]
    }
   ],
   "source": [
    "print(bigram_count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'and the', 'document', 'first', 'first document', 'is', 'is the', 'is this', 'one', 'second', 'second document', 'second second', 'the', 'the first', 'the second', 'the third', 'third', 'third one', 'this', 'this is', 'this the']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>and the</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>first document</th>\n",
       "      <th>is</th>\n",
       "      <th>is the</th>\n",
       "      <th>is this</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>...</th>\n",
       "      <th>second second</th>\n",
       "      <th>the</th>\n",
       "      <th>the first</th>\n",
       "      <th>the second</th>\n",
       "      <th>the third</th>\n",
       "      <th>third</th>\n",
       "      <th>third one</th>\n",
       "      <th>this</th>\n",
       "      <th>this is</th>\n",
       "      <th>this the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  and the  document  first  first document  is  is the  is this  one  \\\n",
       "0    0        0         1      1               1   1       1        0    0   \n",
       "1    0        0         1      0               0   1       1        0    0   \n",
       "2    1        1         0      0               0   0       0        0    1   \n",
       "3    0        0         1      1               1   1       0        1    0   \n",
       "\n",
       "   second  ...  second second  the  the first  the second  the third  third  \\\n",
       "0       0  ...              0    1          1           0          0      0   \n",
       "1       2  ...              1    1          0           1          0      0   \n",
       "2       0  ...              0    1          0           0          1      1   \n",
       "3       0  ...              0    1          1           0          0      0   \n",
       "\n",
       "   third one  this  this is  this the  \n",
       "0          0     1        1         0  \n",
       "1          0     1        1         0  \n",
       "2          1     0        0         0  \n",
       "3          0     1        0         1  \n",
       "\n",
       "[4 rows x 21 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(bigram_count_vectorizer.get_feature_names())\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(bigram_count_vectorizer.fit_transform(text).toarray(), columns=bigram_count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. TfidfVectorizer\n",
    "\n",
    "Word counts are a good starting point, but are very basic. One issue with simple counts is that longer document will have more imapct than small documents. An alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF. This is an acronym than stands for “Term Frequency – Inverse Document”.\n",
    "\n",
    "- Term Frequency: This summarizes how often a given term appears within a document.  \n",
    "- Inverse Document Frequency: This downscales terms that appear a lot across documents.\n",
    "\n",
    "![](tfidf.png)\n",
    "\n",
    "#### TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents.\n",
    "\n",
    "The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. \n",
    "\n",
    "It is interesting to know that their are many variants of calculating tf and idf\n",
    "\n",
    "![](tfidf_formulas.png)\n",
    "\n",
    "Let's use TfidfVectorizer from sklearn and create an instance of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:35:08.784010Z",
     "start_time": "2019-11-16T11:35:08.777800Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create the transform\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the fit() function in order to learn a vocabulary from one or more documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:35:21.207705Z",
     "start_time": "2019-11-16T11:35:21.188039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n"
     ]
    }
   ],
   "source": [
    "# tokenize and build vocab\n",
    "tfidf_vectorizer.fit(text)\n",
    "\n",
    "# summarize\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the transform() function on one or more documents as needed to encode each as a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:35:22.433365Z",
     "start_time": "2019-11-16T11:35:22.426282Z"
    }
   },
   "outputs": [],
   "source": [
    "# encode document\n",
    "tfidf_matrix = tfidf_vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:35:23.463616Z",
     "start_time": "2019-11-16T11:35:23.458345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]\n",
      " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are all the word tokens (without punctuation) in sorted order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:35:24.631641Z",
     "start_time": "2019-11-16T11:35:24.623049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438777</td>\n",
       "      <td>0.541977</td>\n",
       "      <td>0.438777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.358729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.853226</td>\n",
       "      <td>0.222624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.552805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.552805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288477</td>\n",
       "      <td>0.552805</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438777</td>\n",
       "      <td>0.541977</td>\n",
       "      <td>0.438777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.358729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and  document     first        is       one    second       the  \\\n",
       "0  0.000000  0.438777  0.541977  0.438777  0.000000  0.000000  0.358729   \n",
       "1  0.000000  0.272301  0.000000  0.272301  0.000000  0.853226  0.222624   \n",
       "2  0.552805  0.000000  0.000000  0.000000  0.552805  0.000000  0.288477   \n",
       "3  0.000000  0.438777  0.541977  0.438777  0.000000  0.000000  0.358729   \n",
       "\n",
       "      third      this  \n",
       "0  0.000000  0.438777  \n",
       "1  0.000000  0.272301  \n",
       "2  0.552805  0.000000  \n",
       "3  0.000000  0.438777  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tfidf_vectorizer.fit_transform(text).toarray(), columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- https://spacy.io/usage\n",
    "\n",
    "### Appendix\n",
    "\n",
    "I. Annotation tool (also developed by Explosion.AI)\n",
    "\n",
    "- https://prodi.gy/demo?view_id=ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
